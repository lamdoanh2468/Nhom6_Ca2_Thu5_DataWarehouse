import os
import time
import pandas as pd
from datetime import datetime
from playwright.sync_api import sync_playwright
# üëá 1. TH√äM get_connection V√ÄO ƒê√ÇY
from db_connector import log_etl, get_connection 

def crawl_data_from_source():
    process_name = "Crawl_Data_Process"
    
    # üëá 2. TH√äM ƒêO·∫†N CHECK K·∫æT N·ªêI N√ÄY
    print("üîå ƒêang ki·ªÉm tra k·∫øt n·ªëi Database tr∆∞·ªõc khi c√†o...")
    conn_check = get_connection('control')
    if not conn_check:
        print("‚ùå L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi Database Control. H·ªßy b·ªè vi·ªác c√†o d·ªØ li·ªáu.")
        return # D·ª´ng ngay, kh√¥ng m·ªü tr√¨nh duy·ªát
    else:
        print("‚úÖ K·∫øt n·ªëi Database ·ªïn ƒë·ªãnh. Ti·∫øp t·ª•c...")
        conn_check.close() # ƒê√≥ng k·∫øt n·ªëi ki·ªÉm tra
    # ---------------------------------------------------------

    log_etl(process_name, "Running", "B·∫Øt ƒë·∫ßu kh·ªüi ƒë·ªông tr√¨nh duy·ªát ƒë·ªÉ c√†o d·ªØ li·ªáu...")
    
    data_list = []
    
    try:
        with sync_playwright() as p:
            # 1. M·ªü tr√¨nh duy·ªát
            browser = p.chromium.launch(headless=True) # S·ª≠a th√†nh False n·∫øu mu·ªën xem ch·∫°y
            page = browser.new_page()
            
            url = "https://cellphones.com.vn/laptop.html"
            print(f"üï∑Ô∏è [Crawl] ƒêang truy c·∫≠p: {url}")
            page.goto(url, timeout=60000)
            
            # 2. Cu·ªôn trang
            for _ in range(5):
                page.mouse.wheel(0, 5000)
                time.sleep(2)
            
            # 3. L·∫•y danh s√°ch th·∫ª s·∫£n ph·∫©m
            product_items = page.locator(".product-info-container")
            count = product_items.count()
            print(f"üîé [Crawl] T√¨m th·∫•y {count} s·∫£n ph·∫©m tr√™n trang danh s√°ch.")
            
            if count == 0:
                log_etl(process_name, "Warning", "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o tr√™n trang web.")
                browser.close()
                return

            # 4. Duy·ªát qua t·ª´ng s·∫£n ph·∫©m l·∫•y Link
            links_to_scrape = []
            for i in range(min(count, 5)): 
                try:
                    item = product_items.nth(i)
                    name = item.locator(".product__name h3").inner_text()
                    
                    price_locator = item.locator(".product__price--show")
                    if price_locator.count() > 0:
                        price = price_locator.inner_text().replace('‚Ç´', '').replace('.', '').strip()
                    else:
                        price = "0"
                        
                    link_href = item.locator("a").get_attribute("href")
                    
                    if link_href:
                        links_to_scrape.append({
                            "Name": name,
                            "Price": price,
                            "links_href": link_href
                        })
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói l·∫•y item {i}: {e}")
                    continue
            
            print(f"üöÄ [Crawl] B·∫Øt ƒë·∫ßu v√†o chi ti·∫øt {len(links_to_scrape)} s·∫£n ph·∫©m...")

            # 5. V√†o t·ª´ng trang chi ti·∫øt
            for product in links_to_scrape:
                try:
                    print(f"   -> ƒêang xem: {product['Name']}...")
                    page.goto(product['links_href'], timeout=60000)
                    time.sleep(1)
                    
                    def get_text(selector):
                        if page.locator(selector).count() > 0:
                            return page.locator(selector).first.inner_text().strip()
                        return ""

                    product["CpuType"] = get_text("text=Lo·∫°i CPU >> xpath=../following-sibling::div")
                    product["Ram"] = get_text("text=Dung l∆∞·ª£ng RAM >> xpath=../following-sibling::div")
                    product["Storage"] = get_text("text=·ªî c·ª©ng >> xpath=../following-sibling::div")
                    product["Display"] = get_text("text=K√≠ch th∆∞·ªõc m√†n h√¨nh >> xpath=../following-sibling::div")
                    product["GPU"] = get_text("text=Card ƒë·ªì h·ªça >> xpath=../following-sibling::div")
                    product["OSystem"] = get_text("text=H·ªá ƒëi·ªÅu h√†nh >> xpath=../following-sibling::div")
                    product["Battery"] = get_text("text=Pin >> xpath=../following-sibling::div")
                    product["Resolution"] = get_text("text=ƒê·ªô ph√¢n gi·∫£i m√†n h√¨nh >> xpath=../following-sibling::div")
                    
                    data_list.append(product)
                    
                except Exception as e:
                    print(f"‚ùå L·ªói chi ti·∫øt s·∫£n ph·∫©m: {e}")
            
            browser.close()

        # 6. L∆∞u d·ªØ li·ªáu ra File CSV
        if not data_list:
            log_etl(process_name, "Warning", "K·∫øt th√∫c nh∆∞ng kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")
            return

        df = pd.DataFrame(data_list)
        
        # S·ª≠ d·ª•ng os.getcwd() ƒë·ªÉ ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng khi ch·∫°y t·ª´ Scheduler
        # Gi·∫£ s·ª≠ file CrawData.py n·∫±m trong D:\LaptopDW\scripts -> L√πi 1 c·∫•p ƒë·ªÉ ra D:\LaptopDW
        # Tuy nhi√™n, Scheduler ƒë√£ set th∆∞ m·ª•c l√†m vi·ªác l√† D:\LaptopDW r·ªìi, n√™n ta d√πng 'data/raw' tr·ª±c ti·∫øp
        
        raw_path = os.path.join(os.getcwd(), 'data', 'raw')
        
        if not os.path.exists(raw_path):
            os.makedirs(raw_path)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"laptop_{timestamp}.csv"
        full_path = os.path.join(raw_path, filename)

        df.to_csv(full_path, index=False, encoding='utf-8-sig')
        
        msg = f"ƒê√£ l∆∞u {len(df)} d√≤ng v√†o file: {filename}"
        print(f"‚úÖ {msg}")
        log_etl(process_name, "Success", msg, len(df))

    except Exception as e:
        print(f"üî• L·ªói Fatal Crawl: {e}")
        log_etl(process_name, "Failed", str(e))

if __name__ == "__main__":
    crawl_data_from_source()