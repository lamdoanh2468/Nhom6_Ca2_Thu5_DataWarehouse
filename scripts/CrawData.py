import os
import time
import random
import pandas as pd
from datetime import datetime
from playwright.sync_api import sync_playwright
from db_connector import log_etl  # H√†m ghi log chung

def crawl_data_from_source():
    process_name = "Crawl_Data_Process"
    log_etl(process_name, "Running", "B·∫Øt ƒë·∫ßu kh·ªüi ƒë·ªông tr√¨nh duy·ªát ƒë·ªÉ c√†o d·ªØ li·ªáu...")
    
    data_list = []
    
    try:
        with sync_playwright() as p:
            # 1. M·ªü tr√¨nh duy·ªát (headless=True ƒë·ªÉ ch·∫°y ng·∫ßm, False ƒë·ªÉ hi·ªán l√™n xem)
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            url = "https://cellphones.com.vn/laptop.html"
            print(f"üï∑Ô∏è [Crawl] ƒêang truy c·∫≠p: {url}")
            page.goto(url, timeout=60000)
            
            # 2. Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m s·∫£n ph·∫©m (Lazy load)
            for _ in range(5):  # Cu·ªôn 5 l·∫ßn, tƒÉng l√™n n·∫øu mu·ªën l·∫•y nhi·ªÅu h∆°n
                page.mouse.wheel(0, 5000)
                time.sleep(2)
            
            # 3. L·∫•y danh s√°ch th·∫ª s·∫£n ph·∫©m
            product_items = page.locator(".product-info-container")
            count = product_items.count()
            print(f"üîé [Crawl] T√¨m th·∫•y {count} s·∫£n ph·∫©m tr√™n trang danh s√°ch.")
            
            if count == 0:
                log_etl(process_name, "Warning", "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o tr√™n trang web.")
                browser.close()
                return

            # 4. Duy·ªát qua t·ª´ng s·∫£n ph·∫©m ƒë·ªÉ l·∫•y th√¥ng tin s∆° b·ªô & Link
            links_to_scrape = []
            for i in range(min(count, 5)):  # L·∫•y th·ª≠ 20 s·∫£n ph·∫©m ƒë·∫ßu ti√™n ƒë·ªÉ test
                try:
                    item = product_items.nth(i)
                    name = item.locator(".product__name h3").inner_text()
                    
                    # X·ª≠ l√Ω gi√° (c√≥ th·ªÉ c√≥ khuy·∫øn m√£i ho·∫∑c kh√¥ng)
                    price_locator = item.locator(".product__price--show")
                    if price_locator.count() > 0:
                        price = price_locator.inner_text().replace('‚Ç´', '').replace('.', '').strip()
                    else:
                        price = "0"
                        
                    link_href = item.locator("a").get_attribute("href")
                    
                    if link_href:
                        links_to_scrape.append({
                            "Name": name,
                            "Price": price,
                            "links_href": link_href
                        })
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói l·∫•y item {i}: {e}")
                    continue
            
            print(f"üöÄ [Crawl] B·∫Øt ƒë·∫ßu v√†o chi ti·∫øt {len(links_to_scrape)} s·∫£n ph·∫©m...")

            # 5. V√†o t·ª´ng trang chi ti·∫øt ƒë·ªÉ l·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t
            for product in links_to_scrape:
                try:
                    print(f"   -> ƒêang xem: {product['Name']}...")
                    page.goto(product['links_href'], timeout=60000)
                    time.sleep(1) # Ngh·ªâ x√≠u ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                    
                    # L·∫•y b·∫£ng th√¥ng s·ªë k·ªπ thu·∫≠t (Technical Specs)
                    # L∆∞u √Ω: Selector n√†y c√≥ th·ªÉ thay ƒë·ªïi t√πy giao di·ªán web th·ª±c t·∫ø
                    # ƒê√¢y l√† v√≠ d·ª• logic, b·∫°n c·∫ßn F12 tr√™n web ƒë·ªÉ check selector ch√≠nh x√°c
                    
                    # H√†m ph·ª• tr·ª£ l·∫•y text an to√†n
                    def get_text(selector):
                        if page.locator(selector).count() > 0:
                            return page.locator(selector).first.inner_text().strip()
                        return ""

                    # Map d·ªØ li·ªáu (Selector m·∫´u - C·∫ßn ƒëi·ªÅu ch·ªânh theo th·ª±c t·∫ø CellphoneS)
                    product["CpuType"] = get_text("text=Lo·∫°i CPU >> xpath=../following-sibling::div")
                    product["Ram"] = get_text("text=Dung l∆∞·ª£ng RAM >> xpath=../following-sibling::div")
                    product["Storage"] = get_text("text=·ªî c·ª©ng >> xpath=../following-sibling::div")
                    product["Display"] = get_text("text=K√≠ch th∆∞·ªõc m√†n h√¨nh >> xpath=../following-sibling::div")
                    product["GPU"] = get_text("text=Card ƒë·ªì h·ªça >> xpath=../following-sibling::div")
                    product["OSystem"] = get_text("text=H·ªá ƒëi·ªÅu h√†nh >> xpath=../following-sibling::div")
                    product["Battery"] = get_text("text=Pin >> xpath=../following-sibling::div")
                    product["Resolution"] = get_text("text=ƒê·ªô ph√¢n gi·∫£i m√†n h√¨nh >> xpath=../following-sibling::div")
                    
                    data_list.append(product)
                    
                except Exception as e:
                    print(f"‚ùå L·ªói chi ti·∫øt s·∫£n ph·∫©m: {e}")
            
            browser.close()

        # 6. L∆∞u d·ªØ li·ªáu ra File CSV
        if not data_list:
            log_etl(process_name, "Warning", "K·∫øt th√∫c nh∆∞ng kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")
            return

        df = pd.DataFrame(data_list)
        
        # T·∫°o ƒë∆∞·ªùng d·∫´n: D:/LaptopDW/data/raw/
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        raw_path = os.path.join(base_dir, 'data', 'raw')
        
        if not os.path.exists(raw_path):
            os.makedirs(raw_path)

        # T√™n file: laptop_YYYYMMDD_HHMMSS.csv
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"laptop_{timestamp}.csv"
        full_path = os.path.join(raw_path, filename)

        df.to_csv(full_path, index=False, encoding='utf-8-sig')
        
        msg = f"ƒê√£ l∆∞u {len(df)} d√≤ng v√†o file: {filename}"
        print(f"‚úÖ {msg}")
        log_etl(process_name, "Success", msg, len(df))

    except Exception as e:
        print(f"üî• L·ªói Fatal Crawl: {e}")
        log_etl(process_name, "Failed", str(e))

if __name__ == "__main__":
    crawl_data_from_source()